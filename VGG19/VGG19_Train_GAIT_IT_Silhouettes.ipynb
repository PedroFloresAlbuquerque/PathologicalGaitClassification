{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["####### Imports #######\n","# Directory and file paths processing\n","# Image processing into 3D arrays\n","import re\n","import os\n","import sys\n","sys.path.append('/home/pfa/Documents/Code')\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import keras\n","from keras.preprocessing import image\n","from keras.utils import to_categorical\n","from tensorflow.keras.models import load_model\n","from sklearn.metrics import confusion_matrix\n","from sort import sort_nicely\n","from multiprocessing import Process, Manager\n","import pickle\n","import json"]},{"cell_type":"code","metadata":{"id":"sDFPG6oPSmpU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ea7bd65d-b4c4-4de4-d951-e5cc02260332","executionInfo":{"status":"ok","timestamp":1586947678206,"user_tz":-60,"elapsed":2264,"user":{"displayName":"Pedro Albuquerque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizmlZFAEhzGQzIc9I1pMH50aLaInOTVXuxVo-tlQ=s64","userId":"14382357453248489527"}}},"source":["## Get datase\n","base_dir = '/home/datasets/GAIT-IT'\n","metadata_dir = '/home/datasets/metadata'\n","\n","sample_count = 0\n","# all_images = {'train': [], 'validation': [], 'test': []}\n","# all_labels = {'train': [], 'validation': [], 'test': []}\n","subjects_data = {}\n","# train_subjs = ['s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11','s12','s13','s14','s15','s16','s17','s18','s21']\n","# validation_subjs = ['s19','s20']\n","# test_subjs = ['s22','s23']\n","train_images = []; train_labels = []\n","validation_images = []; validation_labels = []\n","test_images = []; test_labels = []\n","\n","classes = {'Diplegic' : 0, 'Hemiplegic' : 1, 'Neuropathic' : 2, 'Normal' : 3, 'Parkinson' : 4}\n","classes_inv = {0 : 'Diplegic', 1 : 'Hemiplegic', 2 : 'Neuropathic', 3 : 'Normal', 4 : 'Parkinson'}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cNn-qyNsSmpZ","colab_type":"code","outputId":"2492f658-9b3a-4e30-b68e-423a65943882","executionInfo":{"status":"ok","timestamp":1586948204346,"user_tz":-60,"elapsed":517899,"user":{"displayName":"Pedro Albuquerque","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GizmlZFAEhzGQzIc9I1pMH50aLaInOTVXuxVo-tlQ=s64","userId":"14382357453248489527"}},"colab":{"base_uri":"https://localhost:8080/","height":102}},"source":["## Train silhouettes directories\n","def sets(train_subjs,validation_subjs,test_subjs):\n","    sample_count = 0\n","    all_images = {'train': [], 'validation': [], 'test': []}\n","    all_labels = {'train': [], 'validation': [], 'test': []}\n","\n","    # Sort pathologies, OCD purposes only\n","    pathologies = list(classes.keys())\n","    sort_nicely(pathologies)\n","    for pathology in pathologies:\n","\n","        pathology_dir = base_dir + '/{}'.format(pathology)\n","        print(pathology_dir)\n","        pathology_subj_folders = [name for name in os.listdir(pathology_dir) if os.path.isdir(os.path.join(pathology_dir, name))]\n","        sort_nicely(pathology_subj_folders)\n","        \n","        # /Pathology/subj{i}/silhouettes/subj_{i}-pat_{j}-lvl_{k}-{l}_{direction}\n","        for subj_folder in pathology_subj_folders:\n","\n","            if subj_folder in train_subjs: subj_set = 'train'\n","            elif subj_folder in validation_subjs: subj_set = 'validation'\n","            elif subj_folder in test_subjs: subj_set = 'test'\n","\n","            subj_folder_dir = os.path.join(pathology_dir, subj_folder)\n","            # print(subj_folder_dir)\n","            subj_silhouettes_dir = os.path.join(subj_folder_dir, 'silhouettes', 'side_view')\n","            # print(subj_silhouettes_dir)\n","\n","            subj_silhouettes_folders = [name for name in os.listdir(subj_silhouettes_dir) if os.path.isdir(os.path.join(subj_silhouettes_dir, name))]\n","            sort_nicely(subj_silhouettes_folders)\n","\n","            folders = [f for f in subj_silhouettes_folders if '_' in f]\n","            for folder in folders:\n","\n","                # Initialize dictionary to store key frames\n","                key_frames = {}\n","\n","                # Directory with metadata about current folder\n","                subj_silhouettes_metadata_dir = subj_silhouettes_dir.replace('GAIT-IT', 'metadata')\n","                subj_pat_metadata = os.path.join(subj_silhouettes_metadata_dir,'metadata/key_frames.json')\n","\n","                with open(subj_pat_metadata) as f:\n","                    key_frames = json.load(f)\n","\n","                # Directory with the sillouettes images\n","                subj_pat_lvl_dir = os.path.join(subj_silhouettes_dir, folder)\n","                # print(subj_pat_lvl_dir)\n","\n","                files = os.listdir(subj_pat_lvl_dir)\n","                sort_nicely(files)\n","                file_names = [files[f] for f in key_frames[folder]]\n","                \n","                # Convert images to numpy arrays, put in batches\n","                # for file_name in file_names:\n","                for i in range(0, len(file_names)-8, 9):\n","                    for j in range(0,9):\n","                        file_path = os.path.join(subj_pat_lvl_dir, file_names[i+j])\n","                        img = image.load_img(file_path, target_size=(224, 224))\n","                        img_tensor = image.img_to_array(img)\n","                        sample_count += 1\n","\n","                        all_images[subj_set].append(img_tensor)\n","                        all_labels[subj_set].append(classes[pathology])\n","    print(len(all_images['train'])); print(len(all_images['validation'])); print(len(all_images['test']))\n","    print(len(all_labels['train'])); print(len(all_labels['validation'])); print(len(all_labels['test']))\n","    \n","    return all_images, all_labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uT8oC2AUSmpn","colab_type":"code","colab":{}},"source":["# Dense Classifier\n","from tensorflow.keras import models, layers, optimizers\n","## Import VGG19 convolutional base\n","from tensorflow.keras.applications import VGG16, VGG19\n","\n","def buildModel():\n","\n","    # Build de VGG19 Convolutional Network base\n","    conv_base = VGG19(weights='imagenet',\n","                      include_top=False,\n","                      input_shape=(224, 224, 3))\n","\n","    # Train convolutional blocks 3, 4 and 5\n","    conv_base.trainable = True\n","\n","    set_trainable = False\n","    for layer in conv_base.layers:\n","        if layer.name == 'block3_conv1':\n","            set_trainable = True\n","        if set_trainable:\n","            layer.trainable = True\n","        else:\n","            layer.trainable = False\n","\n","    # Build the Fully connected layers of the CNN for classification\n","    model = models.Sequential()\n","    model.add(conv_base)\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(4096, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(4096, activation='relu'))\n","    model.add(layers.Dropout(0.5))\n","    model.add(layers.Dense(5, activation='softmax'))\n","\n","    model.compile(optimizer=optimizers.SGD(learning_rate=0.0002, momentum=0.9, nesterov=True),\n","                      loss='categorical_crossentropy',\n","                      metrics = ['acc'])\n","    #model.summary()\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","\n","def create_train(k, all_history, confusion_matrices):\n","\n","    # Define Model Checkpoint callback\n","    checkpointer = ModelCheckpoint(monitor='val_acc', filepath='Train_GAIT_IT_Silhouettes/modelsGEIs/model{}.hdf5'.format(k), verbose=1, save_best_only=True)\n","    early_stopping_criteria = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n","\n","\n","    all_validation_subjs = [['s1','s2','s3'],['s3','s4','s5'],['s5','s6','s7'],['s7','s8','s9'],['s9','s10','s11'],['s11','s12','s14'],['s14','s15','s16'],['s16','s17','s18'],['s18','s19','s20'],['s20','s22','s23']]\n","    validation_subjs = all_validation_subjs[k]\n","    test_subjs = ['s13','s21']\n","    train_subjs = ['s{}'.format(i) for i in range(1,24) if 's{}'.format(i) not in validation_subjs and 's{}'.format(i) not in test_subjs]\n","\n","    print(train_subjs, validation_subjs, test_subjs)\n","\n","    # Split data into training, validation and test sets\n","    all_images, all_labels = sets(train_subjs=train_subjs, validation_subjs=validation_subjs, test_subjs=test_subjs)\n","    \n","    train_images = np.array(all_images['train']); train_labels = to_categorical(np.array(all_labels['train']))\n","    validation_images = np.array(all_images['validation']); validation_labels = to_categorical(np.array(all_labels['validation']))\n","    test_images = np.array(all_images['test']); test_labels = to_categorical(np.array(all_labels['test']))\n","\n","    # Call model creator\n","    model = buildModel()\n","\n","    # Train model using keras.fit\n","    history = model.fit(train_images, train_labels,\n","                        epochs=50,\n","                        batch_size=15,\n","                        validation_data=(validation_images, validation_labels), callbacks=[checkpointer, early_stopping_criteria], verbose=1)\n","    \n","    # Update model history with current model for current validation subject\n","    all_history.append(history.history)\n","\n","    # Load best model from checkpoint of current fold\n","    model = load_model('Train_GAIT_IT_Silhouettes/modelsGEIs/model{}.hdf5'.format(k))\n","    \n","    # # Check validation accuracy\n","    # results = model.evaluate(validation_images, validation_labels)\n","    # print(results)\n","\n","    predictions = model.predict(validation_images)\n","    conf_mat = confusion_matrix(np.argmax(validation_labels, axis=1), np.argmax(predictions, axis=1))\n","    confusion_matrices.append(conf_mat)"]},{"cell_type":"code","execution_count":null,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[],"source":["# Define number iterations for k-folds\n","for k in range(0,10):\n","\n","    # Create list to store models history through k-folds iterations for cross validation\n","    all_history = Manager().list()\n","    confusion_matrices = Manager().list()\n","\n","    # Train model with process to free GPU memory after training\n","    p = Process(target=create_train, args=(k, all_history, confusion_matrices))\n","    p.start()\n","    p.join()\n","\n","    # Conver manager list back to normal list\n","    history = [item for item in all_history]\n","\n","    # Store performance history of each fold\n","    with open('Train_GAIT_IT_Silhouettes/dataGEIs/all_history{}'.format(k), 'wb') as f:\n","        pickle.dump(history,f)\n","\n","    # Convert manager list back to normal list\n","    conf_matrices = [item for item in confusion_matrices]\n","\n","    # Store list with confusion matrices from every fold\n","    with open(\"Train_GAIT_IT_Silhouettes/dataGEIs/confusion_matrices{}\".format(k), 'wb') as f:\n","        pickle.dump(conf_matrices,f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_history = []\n","for i in range(0,10):\n","    with open(\"Train_GAIT_IT_Silhouettes/dataGEIs/all_history{}\".format(i), 'rb') as f:\n","        history = pickle.load(f)\n","        all_history.extend(history)\n","with open(\"Train_GAIT_IT_Silhouettes/dataGEIs/all_history\", 'wb') as f:\n","    pickle.dump(all_history,f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["with open(\"Train_GAIT_IT_Silhouettes/dataGEIs/all_history\", 'rb') as f:\n","    all_history = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["## Plot results\n","print(len(all_history))\n","\n","average_acc = [np.mean([x[\"acc\"][i] for x in all_history]) for i in range(50)]\n","average_loss = [np.mean([x[\"loss\"][i] for x in all_history]) for i in range(50)]\n","average_val_acc = [np.mean([x[\"val_acc\"][i] for x in all_history]) for i in range(50)]\n","average_val_loss = [np.mean([x[\"val_loss\"][i] for x in all_history]) for i in range(50)]\n","\n","plt.plot(range(1, len(average_acc) + 1), average_acc, 'bo', label = 'Training accuracy')\n","plt.plot(range(1, len(average_val_acc) + 1), average_val_acc, 'b', label = 'Testing accuracy')\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.savefig('Train_GAIT_IT_Silhouettes/dataGEIs/acc_history')\n","\n","plt.figure()\n","\n","plt.plot(range(1, len(average_loss) + 1), average_loss, 'bo', label = 'Training loss')\n","plt.plot(range(1, len(average_val_loss) + 1), average_val_loss, 'b', label = 'Testing loss')\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.savefig('Train_GAIT_IT_Silhouettes/dataGEIs/loss_history')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_confusion_matrices = []\n","for i in range(0,10:\n","    with open(\"Train_GAIT_IT_Silhouettes/dataGEIs/confusion_matrices{}\".format(i), 'rb') as f:\n","        confusion_matrices = pickle.load(f)\n","        all_confusion_matrices.extend(confusion_matrices)\n","with open(\"Train_GAIT_IT_Silhouettes/dataGEIs/all_confusion_matrices\", 'wb') as f:\n","    pickle.dump(all_confusion_matrices,f)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evaluate_model(k, confusion_matrices):\n","\n","    all_validation_subjs = [['s1','s2','s3'],['s3','s4','s5'],['s5','s6','s7'],['s7','s8','s9'],['s9','s10','s11'],['s11','s12','s14'],['s14','s15','s16'],['s16','s17','s18'],['s18','s19','s20'],['s20','s22','s23']]\n","    validation_subjs = all_validation_subjs[k]\n","    test_subjs = ['s13','s21']\n","    train_subjs = ['s{}'.format(i) for i in range(1,24) if 's{}'.format(i) not in validation_subjs and 's{}'.format(i) not in test_subjs]\n","\n","    print(train_subjs, validation_subjs, test_subjs)\n","\n","    # Split data into training, validation and test sets\n","    all_images, all_labels = sets(train_subjs=train_subjs, validation_subjs=validation_subjs, test_subjs=test_subjs)\n","    \n","    train_images = np.array(all_images['train']); train_labels = to_categorical(np.array(all_labels['train']))\n","    validation_images = np.array(all_images['validation']); validation_labels = to_categorical(np.array(all_labels['validation']))\n","    test_images = np.array(all_images['test']); test_labels = to_categorical(np.array(all_labels['test']))\n","\n","    # Load best model from checkpoint of current fold\n","    model = load_model('Train_GAIT_IT_Silhouettes/modelsGEIs/model{}.hdf5'.format(k))\n","    \n","    # Check validation accuracy\n","    results = model.evaluate(validation_images, validation_labels)\n","    print(results)\n","\n","    predictions = model.predict(validation_images)\n","    conf_mat = confusion_matrix(np.argmax(validation_labels, axis=1), np.argmax(predictions, axis=1))\n","    confusion_matrices.append(conf_mat)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load best models of each fold and compute confusion matrices\n","confusion_matrices = Manager().list()\n","for k in range(0,10):\n","    p = Process(target=evaluate_model, args=(k, confusion_matrices))\n","    p.start()\n","    p.join()\n","\n","# Convert manager list back to normal list\n","conf_matrices = [item for item in confusion_matrices]\n","\n","# Store list with confusion matrices from every fold\n","with open(\"Train_GAIT_IT_Silhouettes/dataGEIs/confusion_matrices\", 'wb') as f:\n","    pickle.dump(conf_matrices,f)"]},{"cell_type":"code","metadata":{"id":"QbIpeOvkSmp4","colab_type":"code","colab":{}},"source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","# Function to display confusion matrices\n","def DisplayCM(CM, title, save):\n","\n","    display_labels = ['Diplegic', 'Hemiplegic', 'Neuropathic', 'Normal', 'Parkinson']\n","    fig, ax = plt.subplots(figsize=(10, 10))\n","    disp = ConfusionMatrixDisplay(confusion_matrix=CM,\n","                                display_labels=display_labels)\n","    disp = disp.plot(include_values=True,\n","                    cmap='viridis', ax=ax, xticks_rotation='horizontal')\n","    plt.title(title)\n","    plt.tight_layout()\n","\n","    if save: plt.savefig('Train_GAIT_IT_Silhouettes/dataGEIs/Confusion Matrices/' + title)\n","\n","\n","# Initialize normalized and average CMs and normalized CMs list\n","cmAVG = np.zeros((5,5))\n","cmpercent = np.zeros((5,5))\n","cmpercentList = []\n","\n","i = 1\n","# Create list for eachconfusion matrix in percentages\n","for cm in confusion_matrices:\n","    display_labels = ['Diplegic', 'Hemiplegic', 'Neuropathic', 'Normal', 'Parkinson']\n","\n","    cmpercent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","    cmpercentList.append(cmpercent)\n","\n","#    # Plot average CM of current model\n","#     DisplayCM(CM=cmpercent, title='cmpercent{}'.format(i), save=True)\n","#     i += 1\n","\n","\n","for i in range(0,5):\n","    for j in range(0,5):\n","        cmAVG[i][j] = np.mean([x[i][j] for x in cmpercentList])\n","\n","# Plot average CM of current model\n","DisplayCM(CM=cmAVG, title='AVG', save=True)\n","\n","print(cmAVG)\n","print(np.mean([cmAVG[0][0],cmAVG[1][1], cmAVG[2][2], cmAVG[3][3], cmAVG[4][4]]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python35264bite35e49fbf0dc4cf994b6070fccd6f19a","display_name":"Python 3.5.2 64-bit"},"colab":{"name":"VGG19_GAIT_IST_FT.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}