{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Imports #######\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/pfa/Documents/PathologicalGaitClassification')\n",
    "import numpy as np\n",
    "from keras.applications import Xception\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, ConvLSTM2D, GlobalAveragePooling2D, Dropout\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import Nadam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "from sort import sort_nicely\n",
    "from multiprocessing import Process, Manager\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get datase\n",
    "base_dir = '/home/datasets/GAIT-IT'\n",
    "metadata_dir = '/home/datasets/metadata'\n",
    "\n",
    "sample_count = 0\n",
    "all_images = []\n",
    "all_labels = []\n",
    "# subjects_data = {'train': {}, 'validation': {}, 'test': {}}\n",
    "subjects_data = {}\n",
    "# train_subjs = ['s1','s2','s3','s4','s5','s6','s7','s8','s9','s10']\n",
    "# validation_subjs = ['s11','s12']\n",
    "# test_subjs = ['s13','s14']\n",
    "train_images = []; train_labels = []\n",
    "validation_images = []; validation_labels = []\n",
    "test_images = []; test_labels = []\n",
    "\n",
    "classes = {'Diplegic' : 0, 'Hemiplegic' : 1, 'Neuropathic' : 2, 'Normal' : 3, 'Parkinson' : 4}\n",
    "classes_inv = {0 : 'Diplegic', 1 : 'Hemiplegic', 2 : 'Neuropathic', 3 : 'Normal', 4 : 'Parkinson'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Train silhouettes directories\n",
    "\n",
    "# Sort pathologies, OCD purposes only\n",
    "pathologies = list(classes.keys())\n",
    "sort_nicely(pathologies)\n",
    "for pathology in pathologies:\n",
    "\n",
    "    pathology_dir = base_dir + '/{}'.format(pathology)\n",
    "    print(pathology_dir)\n",
    "    pathology_subj_folders = [name for name in os.listdir(pathology_dir) if os.path.isdir(os.path.join(pathology_dir, name))]\n",
    "    sort_nicely(pathology_subj_folders)\n",
    "    \n",
    "    # /Pathology/subj{i}/silhouettes/subj_{i}-pat_{j}-lvl_{k}-{l}_{direction}\n",
    "    for subj_folder in pathology_subj_folders:\n",
    "\n",
    "        if subj_folder not in ['s1','s2','s3','s4','s5','s6']: continue\n",
    "\n",
    "        # if subj_folder in train_subjs: subj_set = 'train'\n",
    "        # elif subj_folder in validation_subjs: subj_set = 'validation'\n",
    "        # elif subj_folder in test_subjs: subj_set = 'test'\n",
    "\n",
    "\n",
    "        subj_folder_dir = os.path.join(pathology_dir, subj_folder)\n",
    "        # print(subj_folder_dir)\n",
    "        subj_silhouettes_dir = os.path.join(subj_folder_dir, 'silhouettes', 'side_view')\n",
    "        # print(subj_silhouettes_dir)\n",
    "\n",
    "        subj_silhouettes_folders = [name for name in os.listdir(subj_silhouettes_dir) if os.path.isdir(os.path.join(subj_silhouettes_dir, name))]\n",
    "        sort_nicely(subj_silhouettes_folders)\n",
    "\n",
    "        if subj_folder not in subjects_data: subjects_data[subj_folder] = {}\n",
    "        subjects_data[subj_folder][pathology] = {}\n",
    "        subjects_data[subj_folder][pathology][\"images\"] = []\n",
    "        subjects_data[subj_folder][pathology][\"labels\"] = []\n",
    "\n",
    "        folders = [f for f in subj_silhouettes_folders if '_' in f]\n",
    "        for folder in folders:\n",
    "\n",
    "            # Initialize dictionary to store key frames\n",
    "            key_frames = {}\n",
    "\n",
    "            # Directory with metadata about current folder\n",
    "            subj_silhouettes_metadata_dir = subj_silhouettes_dir.replace('GAIT-IT-2_silhouettes2', 'metadata')\n",
    "            subj_pat_metadata = os.path.join(subj_silhouettes_metadata_dir,'metadata/key_frames.json')\n",
    "\n",
    "            with open(subj_pat_metadata) as f:\n",
    "                key_frames = json.load(f)\n",
    "\n",
    "            # Directory with the sillouettes images\n",
    "            subj_pat_lvl_dir = os.path.join(subj_silhouettes_dir, folder)\n",
    "            # print(subj_pat_lvl_dir)\n",
    "\n",
    "            files = os.listdir(subj_pat_lvl_dir)\n",
    "            sort_nicely(files)\n",
    "            file_names = [files[f] for f in key_frames[folder]]\n",
    "            \n",
    "            # Convert images to numpy arrays, put in batches\n",
    "            # for file_name in file_names:\n",
    "            for i in range(0, len(file_names), 9):\n",
    "\n",
    "                img_tensors = []\n",
    "                for j in range(0,9):\n",
    "                    file_path = os.path.join(subj_pat_lvl_dir, file_names[i+j])\n",
    "                    img = image.load_img(file_path, target_size=(224, 224))\n",
    "                    img_tensor = image.img_to_array(img)\n",
    "                    img_tensors.append(img_tensor)\n",
    "                    sample_count += 1\n",
    "                \n",
    "                img_tensor_5D = np.stack(img_tensors)\n",
    "                \n",
    "                subjects_data[subj_folder][pathology][\"images\"].append(img_tensor_5D)\n",
    "                subjects_data[subj_folder][pathology][\"labels\"].append(classes[pathology])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def split_data(subjects_data, val_subs, test_subs):\n",
    "\n",
    "    # # Determine K-folds cross validation subject\n",
    "    # validation_subject = 'sub{}'.format(foldIteration)\n",
    "    # print(validation_subject)\n",
    "\n",
    "    # Create training and validation inputs and labels lists\n",
    "    train_images = []; train_labels = []\n",
    "    validation_images = []; validation_labels = []\n",
    "    test_images = []; test_labels = []\n",
    "\n",
    "    # Iterate through subjects data to fill training and validation sets\n",
    "    for subject in subjects_data:\n",
    "        for pathology in subjects_data[subject]:\n",
    "            if subject in val_subs:\n",
    "                validation_images.extend(subjects_data[subject][pathology][\"images\"])\n",
    "                validation_labels.extend(subjects_data[subject][pathology][\"labels\"])\n",
    "            elif subject in test_subs:\n",
    "                test_images.extend(subjects_data[subject][pathology][\"images\"])\n",
    "                test_labels.extend(subjects_data[subject][pathology][\"labels\"])\n",
    "            else:\n",
    "                train_images.extend(subjects_data[subject][pathology][\"images\"])\n",
    "                train_labels.extend(subjects_data[subject][pathology][\"labels\"])\n",
    "\n",
    "    # Convert data lists to arrays\n",
    "    train_images = np.array(train_images)\n",
    "    train_labels = np.array(train_labels)\n",
    "    train_labels = to_categorical(train_labels)\n",
    "    train = {}\n",
    "    train[\"images\"] = train_images\n",
    "    train[\"labels\"] = train_labels\n",
    "\n",
    "    validation_images = np.array(validation_images)\n",
    "    validation_labels = np.array(validation_labels)\n",
    "    validation_labels = to_categorical(validation_labels)\n",
    "    validation = {}\n",
    "    validation[\"images\"] = validation_images\n",
    "    validation[\"labels\"] = validation_labels\n",
    "\n",
    "    # Check if test set was defined\n",
    "    try:\n",
    "        test_images = np.array(test_images)\n",
    "        test_labels = np.array(test_labels)\n",
    "        test_labels = to_categorical(test_labels)\n",
    "        test = {}\n",
    "        test[\"images\"] = test_images\n",
    "        test[\"labels\"] = test_labels\n",
    "    except ValueError:\n",
    "        print(\"Test set is empty\")\n",
    "        test = {}\n",
    "        test[\"images\"] = test_images\n",
    "        test[\"labels\"] = test_labels\n",
    "\n",
    "    # Print total sample count and training and validation set counts\n",
    "    print(\"Total samples: \", sample_count)\n",
    "    print(len(train_images))\n",
    "    print(len(validation_images))\n",
    "    print(len(test_images))\n",
    "\n",
    "    return train, validation, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 224\n",
    "img_width = 224\n",
    "no_of_frames = 9\n",
    "channels = 3\n",
    "no_of_epochs = 50\n",
    "batch_size_value = 5\n",
    "\n",
    "input_video = Input(shape=(no_of_frames, img_width, img_height, channels))\n",
    "cnn_base = Xception(input_shape=(img_width, img_height, channels), weights=\"imagenet\", include_top=False)\n",
    "cnn_base.trainable = False\n",
    "\n",
    "encoded_frames = TimeDistributed(cnn_base)(input_video)\n",
    "encoded_sequence = ConvLSTM2D(64, kernel_size=(7, 7), strides=(2, 2),padding='same', return_sequences=False)(encoded_frames)\n",
    "\n",
    "GAP_layer = GlobalAveragePooling2D()(encoded_sequence)\n",
    "\n",
    "hidden_layer_1 = Dense(activation=\"relu\", units=1024)(GAP_layer)\n",
    "drop_layer=Dropout(0.2)(hidden_layer_1)\n",
    "hidden_layer_2 = Dense(activation=\"relu\", units=512)(drop_layer)\n",
    "outputs = Dense(5, activation=\"softmax\")(hidden_layer_2)\n",
    "model = Model([input_video], outputs)\n",
    "\n",
    "model.summary()\n",
    "nadam_optimizer = Nadam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, schedule_decay=0.004)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=nadam_optimizer, metrics=[\"accuracy\"])\n",
    "\n",
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"Xception_convLSTM2D_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Save the model according to the conditions\n",
    "checkpoint = ModelCheckpoint(\"Xception_convLSTM2D.h5\", monitor='val_acc', verbose=1, save_best_only=True,\n",
    "                            save_weights_only=False,\n",
    "                            mode='auto', period=1)\n",
    "early_stopping_criteria = EarlyStopping(monitor='val_acc', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "# Split data into training, validation and test sets\n",
    "# train, validation, test = split_data(subjects_data=subjects_data, foldIteration=k, test_sub=None)\n",
    "train, validation, test = split_data(subjects_data=subjects_data, val_subs=['s5'] , test_subs=['s6'])\n",
    "train_images = train[\"images\"]; train_labels = train[\"labels\"]\n",
    "validation_images = validation[\"images\"]; validation_labels = validation[\"labels\"]\n",
    "test_images = test[\"images\"]; test_labels = test[\"labels\"]\n",
    "\n",
    "history = model.fit(\n",
    "        train_images, train_labels,\n",
    "        validation_data=(validation_images, validation_labels),\n",
    "        verbose=1,\n",
    "        epochs=no_of_epochs,\n",
    "        callbacks=[checkpoint, early_stopping_criteria])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.evaluate_generator(test_set)\n",
    "print(\"Loss: \", prediction[0], \"Accuracy: \", prediction[1])\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Training', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python35264bite35e49fbf0dc4cf994b6070fccd6f19a",
   "display_name": "Python 3.5.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}